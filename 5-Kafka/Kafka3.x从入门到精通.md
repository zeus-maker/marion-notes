# Kafka3.x从入门到精通

## 第一篇 入门

### 第1章 Kafka概述

- 1.1 定义
- 1.2 消息队列 

	- 1.2.1 传统消息队列的应用场景
	- 1.2.2 消息队列的两种模式

- 1.3 Kafka基础架构

### 第2章 Kafka快速入门

- 2.1 安装部署

	- 2.1.1 集群规划
	- 2.1.2 集群部署
	- 2.1.3 集群启停脚本

- 2.2 Kafka命令行操作

	- 2.2.1 主题命令行操作
	- 2.2.2 生产者命令行操作
	- 2.2.3 消费者命令行操作

### 第3章 Kafka生产者

- 3.1 生产者消息发送流程

	- 3.1.1 发送原理
	- 3.1.2 生产者重要参数列表

- 3.2 异步发送API

	- 3.2.1 普通异步发送
	- 3.2.2 带回调函数的异步发送

	  回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元 数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败
	  

- 3.3 同步发送API
- 3.4 生产者分区

	- 3.4.1 分区好处

		- （1）便于合理使用存储资源，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。
		- （2）提高并行度，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。

	- 3.4.2 生产者发送消息的分区策略

		- 1）默认的分区器 DefaultPartitioner

	- 3.4.3 自定义分区器

- 3.5 生产经验——生产者如何提高吞吐量

	- 1. batch.size：批次大小，默认 16K
	- 2. linger.ms：等待时间，默认 0
	- 3. RecordAccumulator：缓冲区大小，默认 32M：buffer.memory
	- 4.  compression.type：压缩，默认 none，可配置值gzip、snappy、lz4 和 zstd

- 3.6 生产经验——数据可靠性

	- 1）ack 应答原理

	  可靠性总结： 
	  acks=0，生产者发送过来数据就不管了，可靠性差，效率高； 
	  acks=1，生产者发送过来数据Leader应答，可靠性中等，效率中等； 
	  acks=-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低； 
	  在生产环境中，acks=0很少使用；acks=1，一般用于传输普通日志，允许丢个别数据；acks=-1，一般用于传输和钱相关的数据， 对可靠性要求比较高的场景。 
	  

- 3.7 生产经验——数据去重

	- 3.7.1 数据传递语义

	  • 至少一次（At Least Once）= ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2 
	  • 最多一次（At Most Once）= ACK级别设置为0 
	  • 总结： 
	  At Least Once可以保证数据不丢失，但是不能保证数据不重复； 
	  At Most Once可以保证数据不重复，但是不能保证数据不丢失。 
	  • 精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。 
	  Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。 
	  

	- 3.7.2 幂等性

	  幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。 
	  
	  精确一次（Exactly Once） = 幂等性 + 至少一次（ ack=-1 + 分区副本数>=2 + ISR最小副本数量>=2） 。 
	  
	  重复数据的判断标准：具有<PID, Partition, SeqNumber>相同主键的消息提交时，Broker只会持久化一条。其 
	  中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。 
	  所以幂等性只能保证的是在单分区单会话内不重复
	  

	- 3.7.3 生产者事务 - 30

- 3.8 生产经验——数据有序
- 3.9 生产经验——数据乱序

### 第4章 Kafka Broker

- 4.1 Kafka Broker工作流程

	- 4.1.1 Zookeeper存储的Kafka信息
	- 4.1.2 Kafka Broker总体工作流程
	- 4.1.3 Broker重要参数

- 4.2 生产经验——节点服役和退役

	- 4.2.1 服役新节点
	- 4.2.2 退役旧节点

- 4.3 Kafka 副本

	- 4.3.1 副本基本信息

		- （1）Kafka 副本作用：提高数据可靠性。
		- （2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率。
		- （3）Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往 Leader，然后 Follower 找 Leader 进行同步数据。
		- （4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。

	- 4.3.2 Leader选举流程
	- 4.3.3 Leader和Follower故障处理细节
	- 4.3.4 分区副本分配
	- 4.3.5 生产经验——手动调整分区副本存储
	- 4.3.6 生产经验——Leader Partition负载平衡

	- 4.3.7 生产经验——增加副本因子

- 4.4 文件存储

	- 4.4.1 文件存储机制

		- 1）Topic 数据的存储机制

		  Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数 
		  据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制， 
		  将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该 
		  文件夹的命名规则为：topic名称+分区序号，例如：first-0。
		  

		- 2）思考：Topic 数据到底存储在什么位置？
		- 3）index 文件和 log 文件详解

	- 4.4.2 文件清理策略

		- 1）delete 日志删除：将过期数据删除
		- 2）compact 日志压缩

- 4.5 高效读写数据

	- 1）Kafka 本身是分布式集群，可以采用分区技术，并行度高
	- 2）读数据采用稀疏索引，可以快速定位要消费的数据
	- 3）顺序写磁盘
	- 4）页缓存 + 零拷贝技术

### 第5章 Kafka消费者

- 5.1 Kafka消费方式

	- pull（拉）模 式
	- push（推）模式

	  Kafka没有采用这种方式，因为由broker 
	  决定消息发送速率，很难适应所有消费者的 
	  消费速率。例如推送的速度是50m/s， 
	  Consumer1、Consumer2就来不及处理消息。 
	  

- 5.2 Kafka消费者工作流程

	- 5.2.1 消费者总体工作流程
	- 5.2.2 消费者组原理
	- 5.2.3 消费者重要参数

- 5.3 消费者API

	- 5.3.1 独立消费者案例（订阅主题）
	- 5.3.2 独立消费者案例（订阅分区）
	- 5.3.3 消费者组案例

- 5.4 生产经验——分区的分配以及再平衡

	- 5.4.1 Range以及再平衡

	- 5.4.2 RoundRobin以及再平衡

	  RoundRobin 针对集群中所有Topic而言。 
	  RoundRobin 轮询分区策略，是把所有的 partition 和所有的 
	  consumer 都列出来，然后按照 hashcode 进行排序，最后 
	  通过轮询算法来分配 partition 给到各个消费者。
	  

	- 5.4.3 Sticky以及再平衡

	  粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前， 
	  考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。
	  

- 5.5 offset位移

	- 5.5.1 offset的默认维护位置
	- 5.5.2 自动提交offset
	- 5.5.3 手动提交offset

	  手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相 
	  同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成 
	  功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故 
	  有可能提交失败。 
	  

	- 5.5.4 指定Offset消费
	- 5.5.5 指定时间消费
	- 5.5.6 漏消费和重复消费

	  重复消费：已经消费了数据，但是 offset 没提交。 
	  漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。 
	  

- 5.6 生产经验——消费者事务

  如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset 
  过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比 如 
  MySQL）
  

- 5.7 生产经验——数据积压（消费者如何提高吞吐量）

### 第6章 Kafka-Eagle监控

- 6.1 MySQL环境准备
- 6.2 Kafka环境准备
- 6.3 Kafka-Eagle安装
- 6.4 Kafka-Eagle页面操作

### 第7章 Kafka- Kraft模式

- 7.1 Kafka- Kraft架构
- 7.2 Kafka- Kraft集群部署
- 7.3 Kafka- Kraft集群启动停止脚本

## 第二篇 外部系统集成

### 第1章 集成Flume

- 1.1 Flume生产者
- 1.2 Flume消费者

### 第2章 集成Flink

- 2.1 Flink生产者
- 2.2 Flink消费者

### 第3章 集成SpringBoot

- 3.1 SpringBoot生产者
- 3.2 SpringBoot消费者

### 第4章 集成Spark

- 4.1 Spark生产者
- 4.2 Spark消费者

## 第三篇 生产调优手册

### 第1章 Kafka硬件配置选择

- 1.1 场景说明
- 1.2 服务器台数选择
- 1.3 磁盘选择
- 1.4 内存选择
- 1.5 CPU选择
- 1.6 网络选择

### 第2章 Kafka生产者

- 2.1 Kafka生产者核心参数配置
- 2.2 生产者如何提高吞吐量
- 2.3 数据可靠性
- 2.4 数据去重
- 2.5 数据有序
- 2.6 数据乱序

### 第3章 Kafka Broker

- 3.1 Broker核心参数配置
- 3.2 服役新节点/退役旧节点
- 3.3 增加分区
- 3.4 增加副本因子
- 3.5 手动调整分区副本存储
- 3.6 Leader Partition负载平衡
- 3.7 自动创建主题

### 第4章 Kafka消费者

- 4.1 Kafka消费者核心参数配置
- 4.2 消费者再平衡
- 4.3 指定Offset消费
- 4.4 指定时间消费
- 4.5 消费者事务
- 4.6 消费者如何提高吞吐量

### 第5章 Kafka总体

- 5.1 如何提升吞吐量
- 5.2 数据精准一次
- 5.3 合理设置分区数
- 5.4 单条日志大于1m
- 5.5 服务器挂了
- 5.6 集群压力测试

## 第四篇 源码解析

### 第1章 源码环境准备

- 1.1 源码下载地址
- 1.2 安装JDK&Scala
- 1.3 加载源码
- 1.4 安装gradle

### 第2章 生产者源码

- 2.1 初始化

	- 2.1.1 程序入口
	- 2.1.2 生产者main线程初始化
	- 2.1.3 生产者sender线程初始化

- 2.2 发送数据到缓冲区

	- 2.2.1 发送总体流程
	- 2.2.2 分区选择
	- 2.2.3 发送消息大小校验
	- 2.2.4 内存池

- 2.3 sender线程发送数据

### 第3章 消费者源码

- 3.1 初始化

	- 3.1.1 程序入口
	- 3.1.2 消费者初始化

- 3.2 消费者订阅主题
- 3.3 消费者拉取和处理数据

	- 3.3.1 消费者/消费者组初始化
	- 3.3.2 拉取数据
	- 3.3.3 拦截器处理数据

- 3.4 消费者Offset提交

	- 3.4.1 手动同步提交Offset
	- 3.4.2 手动异步提交Offset

### 第4章 服务器源码

- 4.1 程序入口

*XMind - Trial Version*