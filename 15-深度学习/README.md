# 《深度学习之TensorFlow：入门、原理与进阶实战》

## 前言

### 1. 慕课网-机器学习

### 2. 慕课网-深度学习

### 3. 微信读书

## 第1篇 深度学习与TensorFlow基础
（2小时30分钟）

### 第1章 快速了解人工智能与TensorFlow

- 1.1 什么是深度学习
- 1.2 TensorFlow是做什么的
- 1.3 TensorFlow的特点
- 1.4 其他深度学习框架特点及介绍
- 1.5 如何通过本书学好深度学习

### 第2章 搭建开发环境

- 2.1 下载及安装Anaconda开发工具
- 2.2 在Windows平台下载及安装TensorFlow
- 2.3 GPU版本的安装方法
- 2.4 熟悉Anaconda 3开发工具

### 第3章 TensorFlow基本开发步骤——以逻辑回归拟合二维数据为例

- 3.1 实例1：从一组看似混乱的数据中找出y≈2x的规律

    - 3.1.1 准备数据

        - 1．正向搭建模型

            - （1）了解模型及其公式
            - （2）创建模型

        - 2．反向搭建模型

    - 3.1.2 搭建模型
    - 3.1.3 迭代训练模型

        - 1．训练模型
        - 2．训练模型可视化

    - 3.1.4 使用模型

- 3.2 模型是如何训练出来的

    - 3.2.1 模型里的内容及意义
    - 3.2.2 模型内部的数据流向

        - 1．正向
        - 2．反向

- 3.3 了解TensorFlow开发的基本步骤

    - 3.3.1 定义输入节点的方法
    - 3.3.2 实例2：通过字典类型定义输入节点
    - 3.3.3 实例3：直接定义输入节点
    - 3.3.4 定义“学习参数”的变量
    - 3.3.5 实例4：通过字典类型定义“学习参数”
    - 3.3.6 定义“运算”

        - 1．定义正向传播模型
        - 2．定义损失函数

    - 3.3.7 优化函数，优化目标
    - 3.3.8 初始化所有变量
    - 3.3.9 迭代更新参数到最优解
    - 3.3.10 测试模型
    - 3.3.11 使用模型

### 第4章 TensorFlow编程基础

- 4.1 编程模型

    - 4.1.1 了解模型的运行机制
    - 4.1.2 实例5：编写hello world程序演示session的使用
    - 4.1.3 实例6：演示with session的使用
    - 4.1.4 实例7：演示注入机制
    - 4.1.5 建立session的其他方法
    - 4.1.6 实例8：使用注入机制获取节点
    - 4.1.7 指定GPU运算
    - 4.1.8 设置GPU使用资源
    - 4.1.9 保存和载入模型的方法介绍

        - 1．保存模型
        - 2．载入模型

    - 4.1.10 实例9：保存/载入线性回归模型
    - 4.1.11 实例10：分析模型内容，演示模型的其他保存方法

        - 1．模型内容
        - 2．保存模型的其他方法

    - 4.1.12 检查点（Checkpoint）
    - 4.1.13 实例11：为模型添加保存检查点
    - 4.1.14 实例12：更简便地保存检查点
    - 4.1.15 模型操作常用函数总结
    - 4.1.16 TensorBoard可视化介绍
    - 4.1.17 实例13：线性回归的TensorBoard可视化

- 4.2 TensorFlow基础类型定义及操作函数介绍

    - 4.2.1 张量及操作

        - 1．张量介绍

            - （1）tensor类型
            - （2）rank（阶）
            - （3）shape（形状）

        - 2．张量相关操作

            - （1）类型转换
            - （2）数值操作
            - （3）形状变换
            - （4）数据操作

    - 4.2.2 算术运算函数
    - 4.2.3 矩阵相关的运算
    - 4.2.4 复数操作函数
    - 4.2.5 规约计算
    - 4.2.6 分割
    - 4.2.7 序列比较与索引提取
    - 4.2.8 错误类

- 4.3 共享变量

    - 4.3.1 共享变量用途
    - 4.3.2 使用get-variable获取变量
    - 4.3.3 实例14：演示get_variable和Variable的区别

        - 1．Variable的用法
        - 2．get_variable用法演示

    - 4.3.4 实例15：在特定的作用域下获取变量
    - 4.3.5 实例16：共享变量功能的实现
    - 4.3.6 实例17：初始化共享变量的作用域
    - 4.3.7 实例18：演示作用域与操作符的受限范围

- 4.4 实例19：图的基本操作

    - 4.4.1 建立图
    - 4.4.2 获取张量
    - 4.4.3 获取节点操作
    - 4.4.4 获取元素列表
    - 4.4.5 获取对象

- 4.5 配置分布式TensorFlow

    - 4.5.1 分布式TensorFlow的角色及原理
    - 4.5.2 分布部署TensorFlow的具体方法
    - 4.5.3 实例20：使用TensorFlow实现分布式部署训练

        - 1．为每个角色添加IP地址和端口，创建server
        - 2．为ps角色添加等待函数
        - 3．创建网络结构
        - 4．创建Supervisor，管理session
        - 5．迭代训练
        - 6．建立worker文件
        - 7．部署运行

- 4.6 动态图（Eager）
- 4.7 数据集（tf.data）

### 第5章 识别图中模糊的手写数字（实例21）

- 5.1 导入图片数据集

    - 5.1.1 MNIST数据集介绍
    - 5.1.2 下载并安装MNIST数据集

        - 1．利用TensorFlow代码下载MNIST
        - 2．MNIST数据集组成

- 5.2 分析图片的特点，定义变量
- 5.3 构建模型

    - 5.3.1 定义学习参数
    - 5.3.2 定义输出节点
    - 5.3.3 定义反向传播的结构

- 5.4 训练模型并输出中间状态参数
- 5.5 测试模型
- 5.6 保存模型
- 5.7 读取模型

## 第2篇 深度学习基础——神经网络
(6小时)

### 第6章 单个神经元

- 6.1 神经元的拟合原理

    - 6.1.1 正向传播
    - 6.1.2 反向传播

        - 1．BP算法介绍

- 6.2 激活函数——加入非线性因素，解决线性模型缺陷

    - 6.2.1 Sigmoid函数

        - 1．函数介绍
        - 2．在TensorFlow中对应的函数

    - 6.2.2 Tanh函数

        - 1．函数介绍
        - 2．在TensorFlow中对应的函数

    - 6.2.3 ReLU函数

        - 1．函数介绍
        - 2．在TensorFlow中对应的函数

    - 6.2.4 Swish函数
    - 6.2.5 激活函数总结

- 6.3 softmax算法——处理分类问题

    - 6.3.1 什么是softmax
    - 6.3.2 softmax原理
    - 6.3.3 常用的分类函数

- 6.4 损失函数——用真实值与预测值的距离来指导模型的收敛方向

    - 6.4.1 损失函数介绍

        - 1．均值平方差
        - 2．交叉熵
        - 3．总结：损失算法的选取

    - 6.4.2 TensorFlow中常见的loss函数

        - 1．均值平方差
        - 2．交叉熵

- 6.5 softmax算法与损失函数的综合应用

    - 6.5.1 实例22：交叉熵实验
    - 6.5.2 实例23:one_hot实验
    - 6.5.3 实例24:sparse交叉熵的使用
    - 6.5.4 实例25：计算loss值
    - 6.5.5 练习题

- 6.6 梯度下降——让模型逼近最小偏差

    - 6.6.1 梯度下降的作用及分类

        -  批量梯度下降：遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度和更新梯度
        - 随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数，这称为stochasticgradient descent，随机梯度下降
        - 小批量梯度下降：为了克服上面两种方法的缺点，一般采用一种折中手段——小批的梯度下降

    - 6.6.2 TensorFlow中的梯度下降函数
    - 6.6.3 退化学习率——在训练的速度与精度之间找到平衡
    - 6.6.4 实例26：退化学习率的用法举例

- 6.7 初始化学习参数
- 6.8 单个神经元的扩展——Maxout网络

    - 6.8.1 Maxout介绍
    - 6.8.2 实例27：用Maxout网络实现MNIST分类

- 6.9 练习题

### 第7章 多层神经网络——解决非线性问题
（2小时）

- 7.1 线性问题与非线性问题

    - 7.1.1 实例28：用线性单分逻辑回归分析肿瘤是良性还是恶性的

        - 1．生成样本集
        - 2．构建网络结构
        - 3．设置参数进行训练
        - 4．数据可视化
        - 5．线性可分概念

    - 7.1.2 实例29：用线性逻辑回归处理多分类问题

        - 1．生成样本集
        - 2．构建网络结构
        - 3．设置参数进行训练
        - 4．数据可视化
        - 5．线性可分概念

    - 7.1.3 认识非线性问题

- 7.2 使用隐藏层解决非线性问题

    - 7.2.1 实例30：使用带隐藏层的神经网络拟合异或操作

        - 1．数据集介绍
        - 2．网络模型介绍
        - 3．定义变量
        - 4．定义学习参数
        - 5．定义网络模型
        - 6．构建模拟数据
        - 7．运行session，生成结果

    - 7.2.2 非线性网络的可视化及其意义

        - 1．隐藏层神经网络相当于线性可分的高维扩展
        - 2．从逻辑门的角度来理解多层网络

- 7.3 实例31：利用全连接网络将图片进行分类

    - 1．定义网络参数
    - 2．定义网络结构
    - 3．运行session输出结果

- 7.4 全连接网络训练中的优化技巧

    - 7.4.1 实例32：利用异或数据集演示过拟合问题

        - 1．构建异或数据集
        - 2．修改定义网络模型
        - 3．添加可视化
        - 4．欠拟合定义
        - 5．修正模型提高拟合度
        - 6．验证过拟合

    - 7.4.2 正则化

        - 1．什么是正则化
        - 2．TensorFlow中的正则化

    - 7.4.3 实例33：通过正则化改善过拟合情况
    - 7.4.4 实例34：通过增大数据集改善过拟合
    - 7.4.5 练习题
    - 7.4.6 dropout——训练过程中，将部分神经单元暂时丢弃

        - 1．dropout原理
        - 2．TensorFlow中的dropout

    - 7.4.7 实例35：为异或数据集模型添加dropout

        - 构建异或数据集模拟样本，使用多层神经网络将其分类，并使用dropout技术来改善过拟合情况。

    - 7.4.8 实例36：基于退化学习率dropout技术来拟合异或数据集
    - 7.4.9 全连接网络的深浅关系

- 7.5 练习题

### 第8章 卷积神经网络——解决参数太多问题
（1小时30小时）

- 8.1 全连接网络的局限性

    - 1．图像变大导至色彩数变多，不好解决
    - 2．不便处理高维数据

- 8.2 理解卷积神经网络
- 8.3 网络结构

    - 8.3.1 网络结构描述
    - 8.3.2 卷积操作

        - 1．步长
        - 2．窄卷积
        - 3．同卷积
        - 4．全卷积
        - 5．反向传播

            - （1）反向将误差传到前面一层。
            - （2）根据当前的误差对应的学习参数表达式，计算出其需要更新的差值。

        - 6．多通道的卷积

            - 1．均值池化
            - 2．最大池化
            - 3．反向传播

    - 8.3.3 池化层

- 8.4 卷积神经网络的相关函数

    - 8.4.1 卷积函数tf.nn.conv2d
    - 8.4.2 padding规则介绍

        - 1．VALID情况
        - 2．SAME情况
        - 3．规则举例

    - 8.4.3 实例37：卷积函数的使用

        - 1．定义输入变量
        - 2．定义卷积核变量
        - 3．定义卷积操作
        - 4．运行卷积操作

    - 8.4.4 实例38：使用卷积提取图片的轮廓

        - 1．载入图片并显示
        - 2．定义占位符、卷积核、卷积op
        - 3．运行卷积操作并显示

    - 8.4.5 池化函数tf.nn.max_pool（avg_pool）
    - 8.4.6 实例39：池化函数的使用

        - 1．定义输入变量
        - 2．定义池化操作
        - 3．运行池化操作

- 8.5 使用卷积神经网络对图片分类

    - 8.5.1 CIFAR介绍
    - 8.5.2 下载CIFAR数据
    - 8.5.3 实例40：导入并显示CIFAR数据集
    - 8.5.4 实例41：显示CIFAR数据集的原始图片
    - 8.5.5 cifar10_input的其他功能
    - 8.5.6 在TensorFlow中使用queue

        - 1．队列线程启动及挂起机制
        - 2．在session内部的退出机制

    - 8.5.7 实例42：协调器的用法演示
    - 8.5.8 实例43：为session中的队列加上协调器
    - 8.5.9 实例44：建立一个带有全局平均池化层的卷积神经网络

        - 1．导入头文件引入数据集
        - 2．定义网络结构
        - 3．运行session进行训练
        - 4．评估结果

    - 8.5.10 练习题

- 8.6 反卷积神经网络

  反卷积是指，通过测量输出和已知输入重构未知输入的过程。在神经网络中，反卷积过程并不具备学习的能力，仅仅是用于可视化一个已经训练好的卷积网络模型，没有学习训练的过程


	- 8.6.1 反卷积神经网络的应用场景
	- 8.6.2 反卷积原理
	- 8.6.3 实例45：演示反卷积的操作
	- 8.6.4 反池化原理
	- 8.6.5 实例46：演示反池化的操作
	- 8.6.6 实例47：演示gradients基本用法
	- 8.6.7 实例48：使用gradients对多个式子求多变量偏导
	- 8.6.8 实例49：演示梯度停止的实现

- 8.7 实例50：用反卷积技术复原卷积网络各层图像

    - 1．替换Maxpool池化函数
    - 2．反卷积第二层卷积结果
    - 3．反卷积第一层卷积结果
    - 4．合并还原结果，并输出给TensorBoard输出
    - 5．session中写入log
    - 6．Tensorboard中查看结果

- 8.8 善用函数封装库

    - 8.8.1 实例51：使用函数封装库重写CIFAR卷积网络

        - 1．改写代码
        - 2．tf.contrib.layers中的具体函数介绍

    - 8.8.2 练习题

- 8.9 深度学习的模型训练技巧

    - 8.9.1 实例52：优化卷积核技术的演示
    - 8.9.2 实例53：多通道卷积技术的演示
    - 8.9.3 批量归一化

        - 1．批量归一化介绍
        - 2．批量归一化定义
        - 3．批量归一化的简单用法

    - 8.9.4 实例54：为CIFAR图片分类模型添加BN

        - 1．添加BN函数
        - 2．为BN函数添加占位符参数
        - 3．修改网络结构添加BN层
        - 4．加入退化学习率

        - 5．在运行session中添加训练标志

    - 8.9.5 练习题

### 第9章 循环神经网络——具有记忆功能的网络
（2小时）

- 9.1 了解RNN的工作原理

    - 9.1.1 了解人的记忆原理
    - 9.1.2 RNN网络的应用领域

      对于序列化的特征的任务，都适合采用RNN网络来解决。细分起来可以有情感分析（SentimentAnalysis）、关键字提取（Key Term Extraction）、语音识别（Speech Recognition）、机器翻译（Machine Translation）和股票分析等。


	- 9.1.3 正向传播过程

		- （1）开始时t1通过自己的输入权重和0作为输入，生成了out1。
		- （2）out1通过自己的权重生成了h1，然后和t2经过输入权重转化后一起作为输入，生成了out2。
		- （3）out2通过同样的隐藏层权重生成了h2，然后和t3经过输入权重转化后一起作为输入，生成了out2。

	- 9.1.4 随时间反向传播

- 9.2 简单RNN

    - 9.2.1 实例55：简单循环神经网络实现——裸写一个退位减法器

        - 1．定义基本函数
        - 2．建立二进制映射
        - 3．定义参数
        - 4．准备样本数据
        - 5．模型初始化
        - 6．正向传播
        - 7．反向训练
        - 8．输出结果

    - 9.2.2 实例56：使用RNN网络拟合回声信号序列

        - 1．定义参数生成样本数据
        - 2．定义占位符处理输入数据
        - 3．定义网络结构
        - 4．建立session训练数据
        - 5．测试模型及可视化

- 9.3 循环神经网络（RNN）的改进

    - 9.3.1 LSTM网络介绍

        - 1．整体介绍
        - 2．忘记门
        - 3．输入门
        - 4．输出门

    - 9.3.2 窥视孔连接（Peephole）
    - 9.3.3 带有映射输出的STMP
    - 9.3.4 基于梯度剪辑的cell
    - 9.3.5 GRU网络介绍
    - 9.3.6 Bi-RNN网络介绍
    - 9.3.7 基于神经网络的时序类分类CTC

- 9.4 TensorFlow实战RNN


	- 9.4.1 TensorFlow中的cell类
	- 9.4.2 通过cell类构建RNN

		- 1．静态RNN构建
		- 2．动态RNN构建
		- 3．双向RNN构建
		- 4．使用动态RNN处理变长序列

	- 9.4.3 实例57：构建单层LSTM网络对MNIST数据集分类
	- 9.4.4 实例58：构建单层GRU网络对MNIST数据集分类
	- 9.4.5 实例59：创建动态单层RNN网络对MNIST数据集分类
	- 9.4.6 实例60：静态多层LSTM对MNIST数据集分类
	- 9.4.7 实例61：静态多层RNN-LSTM连接GRU对MNIST数据集分类
	- 9.4.8 实例62：动态多层RNN对MNIST数据集分类
	- 9.4.9 练习题
	- 9.4.10 实例63：构建单层动态双向RNN对MNIST数据集分类
	- 9.4.11 实例64：构建单层静态双向RNN对MNIST数据集分类
	- 9.4.12 实例65：构建多层双向RNN对MNIST数据集分类
	- 9.4.13 实例66：构建动态多层双向RNN对MNIST数据集分类
	- 9.4.14 初始化RNN

		- 1．初始化为0
		- 2．初始化为指定值

	- 9.4.15 优化RNN

		- 1．dropout功能
		- 2．LN基于层的归一化

	- 9.4.16 实例67：在GRUCell中实现LN

		- （1）新加了一个函数LN用于做归一化处理。
		- （2）定义一个LNGRU来代替原始的GRU。

	- 9.4.17 CTC网络的loss——ctc_loss

		- 1．ctc_loss函数介绍
		- 2．SparseTensor类型
		- 3．生成SparseTensor
		- 4．SparseTensor转dense
		- 5．levenshtein距离

	- 9.4.18 CTCdecoder

		- 1．CTCdecoer介绍
		- 2．CTCdecoder函数

- 9.5 实例68：利用BiRNN实现语音识别

    - 9.5.1 语音识别背景

      使用神经网络技术可以将语音识别变得简单。通过能进行时序分类的连接时间分类（ConnectionistTemporal Classification, CTC）目标函数，计算多个标签序列的概率，而序列是语音样本中所有可能的对应文字的集合。随后把预测结果与实际进行比较，计算预测结果的误差，以在训练中不断更新网络权重。这样可以丢弃音素的概念，自然也不需要人工根据时序标注对应的音素了。由于是直接拿音频序列来对应文字，连语言模型都可以省去，这样就脱离了标准的语言模型与声学模型，将使语音识别技术与语言无关（也就是中文、英文、地方语言），只要样本足够多，就可以训练出来。


	- 9.5.2 获取并整理样本

		- 1．样本下载
		- 2．样本读取
		- 3．建立批次获取样本函数
		- 4．安装python_speech_features工具
		- 5．提取音频数据MFCC特征
		- 6．批次音频数据对齐
		- 7．文字样本的转化
		- 8．密集矩阵转成稀疏矩阵
		- 9．将字向量转成文字

	- 9.5.3 训练模型

		- 1．定义占位符
		- 2．构建网络模型
		- 3．定义损失函数即优化器
		- 4．定义解码并评估模型节点
		- 5．建立session并添加检查点处理
		- 6．通过循环来迭代训练模型
		- 7．定期评估模型，输出模型解码结果

	- 9.5.4 练习题

- 9.6 实例69：利用RNN训练语言模型

    - 9.6.1 准备样本

        - 1．定义基本工具函数
        - 2．样本预处理

    - 9.6.2 构建模型

        - 1．设置参数定义占位符
        - 2．定义网络结构
        - 3．定义优化器
        - 4．训练模型
        - 5．运行模型生成句子

- 9.7 语言模型的系统学习

    - 9.7.1 统计语言模型
    - 9.7.2 词向量

        - 1．词向量解释
        - 2．词向量训练
        - 3．候选采样技术
        - 4．词向量的应用

    - 9.7.3 word2vec

        - 1．CBOW&Skip-Gram
        - 2．TensorFlow的word2vec

    - 9.7.4 实例70：用CBOW模型训练自己的word2vec

        - 1．引入头文件
        - 2．准备样本创建数据集
        - 3．获取批次数据
        - 4．定义取样参数
        - 5．定义模型变量
        - 6．定义损失函数和优化器
        - 7．夹角余弦介绍
        - 8．启动session，训练模型
        - 9．输入验证数据，显示效果
        - 10．词向量可视化

    - 9.7.5 实例71：使用指定侯选采样本训练word2vec

        - 1．修改字典处理部分，生成词频数据
        - 2．通过词频数据进行候选样本采样
        - 3．使用自己的采样计算softmax的loss
        - 4．运行生成结果

    - 9.7.6 练习题

- 9.8 处理Seq2Seq任务

  本节继续介绍RNN的使用场景，处理Seq2Seq任务。Seq2Seq任务，即从一个序列映射到另一个序列的任务。在生活中会有很多符合这样特性的例子：前面的语言模型、语音识别例子，都可以理解成一个Seq2Seq的例子，类似的应用还有机器翻译、词性标注、智能对话等。下面就来学一下Seq2Seq任务的处理方法。


	- 9.8.1 Seq2Seq任务介绍
	- 9.8.2 Encoder-Decoder框架

		- 1．Encoder-Decoder框架介绍
		- 2．TensorFlow中的Seq2Seq

	- 9.8.3 实例72：使用basic_rnn_seq2seq拟合曲线

		- 1．定义模拟样本函数
		- 2．定义参数及网络结构
		- 3．定义loss函数及优化器
		- 4．启用session开始训练
		- 5．准备可视化数据
		- 6．画图显示数据

	- 9.8.4 实例73：预测当天的股票价格

		- 1．准备数据
		- 2．导入股票数据
		- 3．生成样本
		- 4．运行程序查看效果

	- 9.8.5 基于注意力的Seq2Seq

		- 1．attention_seq2seq介绍
		- 2．TensorFlow中的attention_seq2seq
		- 3．Seq2Seq中桶（bucket）的实现机制

	- 9.8.6 实例74：基于Seq2Seq注意力模型实现中英文机器翻译

		- 1．样本准备
		- 2．生成中、英文字典
		- 3．将数据转成索引格式
		- 4．对样本文件进行分析图示
		- 5．载入字典准备训练
		- 6．启动session，创建模型并读取样本数据
		- 7．通过循环进行训练
		- 8．网络模型Seq2SeqModel的初始化
		- 9．自定义损失函数
		- 10．定义Seq2Seq框架结构
		- 11．定义Seq2seq模型的输入占位符
		- 12．定义正向的输出与loss
		- 13．反向传播计算梯度并通过优化器更新
		- 14．按批次获取样本数据
		- 15．Seq2Seq框架的迭代更新处理
		- 16．测试模型

- 9.9 实例75：制作一个简单的聊天机器人

  实例74中的Seq2Seq模型的代码可以作为很好的框架来扩展使用，简单地改变一下数据样本，即可扩展到许多更有意思的应用中。例如，让机器人对对联、讲故事、生成文章摘要、汉语翻译成英语、聊天机器人等都可以实现。这些扩展应用基本上不需要改动太多的代码就可以完成，本节以聊天机器人来举例演示。


	- 9.9.1 构建项目框架
	- 9.9.2 准备聊天样本
	- 9.9.3 预处理样本
	- 9.9.4 训练样本
	- 9.9.5 测试模型

- 9.10 时间序列的高级接口TFTS

  TFTS的具体接口在tf.contrib.timeseries下。它支持非线性自动回归模型（估算器：ARRegressor）、基于线性状态空间建模的组件集合模型（包括趋势、预测、向量自回归、移动平均值等，估算器：StructuralEnsembleRegressor）、自定义LSTM模型。


### 第10章 自编码网络——能够自学习样本特征的网络
（30小时）

- 10.1 自编码网络介绍及应用

  人们平时看一幅图时，并不是像计算机那个逐个像素去读，一般是扫一眼物体，大致能得到需要的信息，如形状、颜色和特征等。那么怎样让机器也有这项能力呢？
  这里就为大家介绍一下自编码网络。自编码网络是非监督学习领域中的一种，可以自动从无标注的数据中学习特征，是一种以重构输入信号为目标的神经网络，它可以给出比原始数据更好的特征描述，具有较强的特征学习能力，在深度学习中常用自编码网络生成的特征来取代原始数据，以得到更好的结果。


- 10.2 最简单的自编码网络
- 10.3 自编码网络的代码实现

    - 10.3.1 实例76：提取图片的特征，并利用特征还原图片

        - 1．引入头文件，并加载MNIST数据
        - 2．定义网络模型
        - 3．开始训练
        - 4．测试模型
        - 5．双比输入和输出

    - 10.3.2 线性解码器
    - 10.3.3 实例77：提取图片的二维特征，并利用二维特征还原图片

        - 1．引入头文件，定义学习参数变量
        - 2．定义网络模型
        - 3．开始训练
        - 4．对比输入和输出
        - 5．显示数据的二维特征

    - 10.3.4 实例78：实现卷积网络的自编码

        - 1．修改网络权重定义
        - 2．改变编码和解码结构
        - 3．测试及可视化部分改动

    - 10.3.5 练习题

- 10.4 去噪自编码

  要想取得好的特征只靠重构输入数据是不够的，在实际应用中，还需要让这些特征具有抗干扰的能力，即当输入数据发生一定程度的扰动时，生成的特征仍然保持不变。这时需要添加噪声来为模型增加更大的困难。在这种情况下训练出来的模型才会有更好的鲁棒性，于是就有了本节所介绍的去噪自动编码器。


- 10.5 去噪自编码网络的代码实现

    - 10.5.1 实例79：使用去噪自编码网络提取MNIST特征

        - 1．引入头文件，创建网络模型及定义学习参数变量
        - 2．设置训练参数，开始训练
        - 3．生成噪声数据
        - 4．数据可视化
        - 5．测试鲁棒性

    - 10.5.2 练习题

- 10.6 栈式自编码

    - 10.6.1 栈式自编码介绍
    - 10.6.2 栈式自编码在深度学习中的意义

- 10.7 深度学习中自编码的常用方法

    - 10.7.1 代替和级联
    - 10.7.2 自编码的应用场景

- 10.8 去噪自编码与栈式自编码的综合实现

    - 10.8.1 实例80：实现去噪自编码

        - 1．引入头文件，创建网络模型及定义学习参数变量
        - 2．定义占位符
        - 3．定义学习参数
        - 4．第1层网络结构
        - 5．第2层网络结构
        - 6．第3层网络结构
        - 7．定义级联网络结构
        - 8．第1层网络训练
        - 9．第2层网络训练
        - 10．第3层网络训练
        - 11．栈式自编码网络验证
        - 12．级联微调

    - 10.8.2 实例81：添加模型存储支持分布训练
    - 10.8.3 小心分布训练中的“坑”
    - 10.8.4 练习题

- 10.9 变分自编码

    - 10.9.1 什么是变分自编码

      变分自编码，其实就是在编码过程中改变了样本的分布（“变分”可以理解为改变分布）。前面所说的“学习样本的规律”，具体指的就是样本的分布，假设我们知道样本的分布函数，就可以从这个函数中随便取一个样本，然后进行网络解码层前向传导，这样就可以生成一个新的样本。

      为了得到这个样本的分布函数，模型训练的目的将不再是样本本身，而是通过加一个约束项，将网络生成一个服从于高斯分布的数据集，这样按照高斯分布里的均值和方差规则就可以任意取相关的数据，然后通过解码层还原成样本。


	- 10.9.2 实例82：使用变分自编码模拟生成MNIST数据

		- 1．引入库，定义占位符
		- 2．定义学习参数
		- 3．定义网络结构
		- 4．构建模型的反向传播
		- 5．设置参数，进行训练
		- 6．高斯分布取样，生成模拟数据

	- 10.9.3 练习题

- 10.10 条件变分自编码

    - 10.10.1 什么是条件变分自编码

      在变分自编码的基础上，再去理解条件变分自编码会很容易。主要的改动是，在训练、测试时加入一个one-hot向量，用于表示标签向量。其实，就是给变分自编码网络加入了一个条件，让网络学习图片分布时加入了标签因素，这样可以按照标签的数值生成指定的图片。


	- 10.10.2 实例83：使用标签指导变分自编码网络生成MNIST数据

		- 1．添加标签占位符
		- 2．添加输入全连接权重
		- 3．修改模型，将标签输出接入编码
		- 4．修改模型将标签接入解码
		- 5．修改session中的feed部分
		- 6．运行模型生成模拟数据

## 第3篇 深度学习进阶

### 第11章 深度神经网络

本章开始学习深度神经网络的知识。作为深度学习的代表，深度神经网络可以算是深度学习中最主要的知识。前面讲了许多网络形态，都会在各自的领域中有一定的效果，但是要体现出真正的人工智能能力，就必须将这些网络形态组合起来，利用各种网络的优势，使整体效果达到最优，实现可以匹配人工智能的要求。


- 11.1 深度神经网络介绍

    - 11.1.1 深度神经网络起源
    - 11.1.2 经典模型的特点介绍

        - 1．VGG模型
        - 2．GoogLeNet模型
        - 3．ResNet模型
        - 4．Inception-ResNet-v2模型

- 11.2 GoogLeNet模型介绍

    - 11.2.1 MLP卷积层

- 11.3 残差网络（ResNet）
- 11.4 Inception-ResNet-v2结构
- 11.5 TensorFlow中的图片分类模型库——slim
- 11.6 使用slim中的深度网络模型进行图像的识别与检测
- 11.7 实物检测模型库——Object Detection API
- 11.8 实物检测领域的相关模型
- 11.9 机器自己设计的模型（NASNet）

### 第12章 对抗神经网络（GAN）

对抗神经网络其实是两个网络的的组合，可以理解为一个网络生成模拟数据，另一个网络判断生成的数据是真实的还是模拟的。生成模拟数据的网络要不断优化自己让判别的网络判断不出来，判别的网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗神经网络。


- 12.1 GAN的理论知识
- 12.2 DCGAN——基于深度卷积的GAN
- 12.3 InfoGAN和ACGAN：指定类别生成模拟样本的GAN
- 12.4 AEGAN：基于自编码器的GAN
- 12.5 WGAN-GP：更容易训练的GAN
- 12.6 LSGAN（最小乘二GAN）：具有WGAN同样效果的GAN
- 12.7 GAN-cls：具有匹配感知的判别器
- 12.8 SRGAN——适用于超分辨率重建的GAN
- 12.9 GAN网络的高级接口TFGAN
- 12.10 总结

*XMind - Trial Version*